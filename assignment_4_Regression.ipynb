{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " assignment_4_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saisuresh1999/Assignment/blob/master/assignment_4_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVhCarMwG70",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXJ1XpSwdvR",
        "colab_type": "text"
      },
      "source": [
        "Install and import all the necessary libraries for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMrFD-ZBwK",
        "colab_type": "code",
        "outputId": "8ecbf456-a2f4-4d4c-89ec-2d73c9fa0180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import feature_column\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-rc0 in /usr/local/lib/python3.6/dist-packages (2.0.0rc0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0a20190806)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGgAUOKwsWA",
        "colab_type": "text"
      },
      "source": [
        "### **Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe2azQOdmND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gD5esSxfxjs",
        "colab_type": "code",
        "outputId": "85556bb6-2e03-41e3-9f1b-68d06e37a608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323 train examples\n",
            "81 validation examples\n",
            "102 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZTeC55HxDeT",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4GRPPLdTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZy7p3AaTRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KuTr4sxMl6",
        "colab_type": "text"
      },
      "source": [
        "### Defining Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380jHjPokFUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "\n",
        "feature_columns = []\n",
        "for header in ['CRIM'\t,'ZN',\t'INDUS',\t'CHAS'\t,'NOX'\t,'RM'\t,'AGE'\t,'DIS'\t\t,'TAX'\t,'PTRATIO',\t'B',\t'LSTAT']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "rad = feature_column.numeric_column(\"RAD\")\n",
        "rad_buckets = feature_column.bucketized_column(rad, boundaries= [2, 5] )\n",
        "feature_columns.append(rad_buckets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVCMrdMxVB5",
        "colab_type": "text"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAc9LpVzqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6B9FgRyyGXe",
        "colab_type": "text"
      },
      "source": [
        "Model should contain following layers:\n",
        "\n",
        "```\n",
        "feature_layer\n",
        "\n",
        "Dense(1, activation=None)\n",
        "```\n",
        "\n",
        "Use 'Adam' optimizer\n",
        "\n",
        "Use 'mse' as your loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInuZ8D0xsu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile your model in this cell.\n",
        " model = keras.Sequential([\n",
        "    feature_layer,\n",
        "     layers.Dense(1,activation=None)\n",
        "     \n",
        "  ])\n",
        "\n",
        "model.compile(loss='mse',\n",
        "                optimizer='adam',\n",
        "                metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzwKEhjZXLIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdzl3wasRo6",
        "colab_type": "code",
        "outputId": "ad9ef773-83ac-41da-b904-b339e3742e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 1224.8395 - mse: 1155.9988 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 888.5443 - mse: 895.9940 - val_loss: 718.4260 - val_mse: 743.8795\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 787.3161 - mse: 783.1119 - val_loss: 640.0608 - val_mse: 675.0557\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 703.2051 - mse: 720.3595 - val_loss: 581.2295 - val_mse: 613.6686\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 653.1114 - mse: 653.7393 - val_loss: 519.4702 - val_mse: 544.9796\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 589.7511 - mse: 588.1520 - val_loss: 467.8004 - val_mse: 488.3029\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 531.1200 - mse: 531.1484 - val_loss: 417.3567 - val_mse: 436.7618\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 485.9564 - mse: 482.5317 - val_loss: 374.5685 - val_mse: 389.9478\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 438.0851 - mse: 434.7508 - val_loss: 333.8528 - val_mse: 348.3055\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 394.3628 - mse: 392.6158 - val_loss: 297.8722 - val_mse: 312.5504\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 359.2300 - mse: 359.1625 - val_loss: 266.2788 - val_mse: 280.4274\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 325.6223 - mse: 325.0358 - val_loss: 240.0282 - val_mse: 250.5160\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 293.4763 - mse: 295.0803 - val_loss: 219.0092 - val_mse: 226.6849\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 267.7006 - mse: 266.5595 - val_loss: 194.7893 - val_mse: 201.6336\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 240.6825 - mse: 241.6522 - val_loss: 172.5907 - val_mse: 179.2902\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 220.4748 - mse: 218.8224 - val_loss: 152.6123 - val_mse: 160.5190\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 199.6048 - mse: 198.9996 - val_loss: 139.3494 - val_mse: 146.3667\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 176.8938 - mse: 182.8811 - val_loss: 128.3256 - val_mse: 134.6010\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 171.1484 - mse: 170.2493 - val_loss: 121.2370 - val_mse: 126.3061\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 158.7792 - mse: 158.3002 - val_loss: 111.2782 - val_mse: 116.5411\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 147.3582 - mse: 146.5526 - val_loss: 102.0406 - val_mse: 108.0755\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 137.7224 - mse: 137.9854 - val_loss: 96.5679 - val_mse: 102.7151\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 129.9300 - mse: 130.6099 - val_loss: 92.2803 - val_mse: 98.1517\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 126.0566 - mse: 125.1380 - val_loss: 88.5961 - val_mse: 94.8260\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 120.8456 - mse: 120.3444 - val_loss: 85.7767 - val_mse: 91.7695\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 115.1289 - mse: 115.1582 - val_loss: 84.0040 - val_mse: 89.7820\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 111.0885 - mse: 111.5500 - val_loss: 81.6248 - val_mse: 87.5188\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 108.7888 - mse: 107.9017 - val_loss: 79.6366 - val_mse: 85.6512\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 103.7659 - mse: 105.0293 - val_loss: 77.6025 - val_mse: 83.9203\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 102.9211 - mse: 102.6731 - val_loss: 76.3793 - val_mse: 82.9276\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 100.9801 - mse: 100.4412 - val_loss: 75.4277 - val_mse: 81.9392\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 98.5663 - mse: 98.0370 - val_loss: 74.6582 - val_mse: 81.2605\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 96.6239 - mse: 96.4195 - val_loss: 74.3331 - val_mse: 80.9594\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 95.0356 - mse: 94.8220 - val_loss: 73.3536 - val_mse: 80.2012\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 93.9498 - mse: 93.4174 - val_loss: 72.8403 - val_mse: 79.8484\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 92.2104 - mse: 92.3278 - val_loss: 72.4666 - val_mse: 79.4670\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 91.9188 - mse: 91.5961 - val_loss: 71.9698 - val_mse: 79.0371\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 91.1706 - mse: 90.7589 - val_loss: 71.7394 - val_mse: 78.8184\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 90.0160 - mse: 89.3634 - val_loss: 71.3656 - val_mse: 78.5311\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 88.2556 - mse: 88.5787 - val_loss: 71.1830 - val_mse: 78.3909\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 88.2800 - mse: 87.6201 - val_loss: 70.5592 - val_mse: 77.8457\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 86.9526 - mse: 87.1963 - val_loss: 70.3047 - val_mse: 77.6018\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 86.6076 - mse: 86.5972 - val_loss: 70.8956 - val_mse: 78.1762\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 86.2595 - mse: 85.6479 - val_loss: 69.6895 - val_mse: 77.0695\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 83.4793 - mse: 84.8396 - val_loss: 69.5289 - val_mse: 76.9211\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 82.2893 - mse: 84.4541 - val_loss: 69.9140 - val_mse: 77.3354\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 84.1381 - mse: 84.3820 - val_loss: 68.7440 - val_mse: 76.2465\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 84.6867 - mse: 84.1565 - val_loss: 68.1329 - val_mse: 75.6442\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 79.9135 - mse: 82.0115 - val_loss: 68.3862 - val_mse: 75.8315\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 82.3311 - mse: 81.7257 - val_loss: 68.4453 - val_mse: 75.8869\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 81.6043 - mse: 81.2798 - val_loss: 67.2874 - val_mse: 74.7905\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 79.9705 - mse: 80.6836 - val_loss: 66.8241 - val_mse: 74.3638\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 80.4419 - mse: 79.8634 - val_loss: 66.7033 - val_mse: 74.2994\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 79.4566 - mse: 79.4289 - val_loss: 66.4443 - val_mse: 74.0291\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 79.3292 - mse: 78.8892 - val_loss: 67.9245 - val_mse: 75.4994\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 79.1142 - mse: 78.7677 - val_loss: 65.9689 - val_mse: 73.5843\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 77.9137 - mse: 77.6290 - val_loss: 65.3758 - val_mse: 73.0208\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 78.2287 - mse: 77.6170 - val_loss: 65.0309 - val_mse: 72.6671\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 77.0566 - mse: 76.8008 - val_loss: 65.2310 - val_mse: 72.8379\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 76.7717 - mse: 77.1279 - val_loss: 65.0533 - val_mse: 72.6543\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 77.2296 - mse: 76.8006 - val_loss: 63.9359 - val_mse: 71.4986\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 76.3919 - mse: 75.7801 - val_loss: 63.4687 - val_mse: 70.9516\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 75.7594 - mse: 75.1615 - val_loss: 63.6353 - val_mse: 71.0802\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 75.2858 - mse: 74.7339 - val_loss: 62.7716 - val_mse: 70.2154\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 74.8447 - mse: 74.2970 - val_loss: 62.5789 - val_mse: 70.0168\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 74.4599 - mse: 73.8755 - val_loss: 62.2272 - val_mse: 69.6649\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 73.7334 - mse: 73.4890 - val_loss: 61.8685 - val_mse: 69.2795\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 73.8880 - mse: 73.4066 - val_loss: 61.3535 - val_mse: 68.7512\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 72.7164 - mse: 72.8088 - val_loss: 61.0944 - val_mse: 68.4411\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 71.9318 - mse: 72.6319 - val_loss: 60.4638 - val_mse: 67.7572\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 72.6927 - mse: 72.2964 - val_loss: 60.1945 - val_mse: 67.5060\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 70.9000 - mse: 71.6048 - val_loss: 60.8749 - val_mse: 68.1493\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 72.1410 - mse: 71.7534 - val_loss: 59.7725 - val_mse: 67.0875\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 71.1029 - mse: 70.6209 - val_loss: 59.8959 - val_mse: 67.1644\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 70.5649 - mse: 70.2623 - val_loss: 59.4347 - val_mse: 66.7094\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 67.9781 - mse: 70.0068 - val_loss: 59.1779 - val_mse: 66.4483\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 67.6196 - mse: 69.3699 - val_loss: 58.9752 - val_mse: 66.2326\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 71.3861 - mse: 70.9523 - val_loss: 58.7889 - val_mse: 66.0366\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 68.6334 - mse: 69.0243 - val_loss: 58.5861 - val_mse: 65.7952\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 69.6282 - mse: 69.1261 - val_loss: 60.0548 - val_mse: 67.2624\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 68.6962 - mse: 68.6612 - val_loss: 58.1284 - val_mse: 65.3162\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 69.3159 - mse: 68.8376 - val_loss: 57.4722 - val_mse: 64.6729\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 68.0056 - mse: 67.6255 - val_loss: 57.2656 - val_mse: 64.4539\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 66.3617 - mse: 67.0465 - val_loss: 57.4331 - val_mse: 64.5833\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 69.1712 - mse: 68.8774 - val_loss: 59.0392 - val_mse: 66.1926\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 68.9600 - mse: 68.7251 - val_loss: 56.3658 - val_mse: 63.4827\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 66.5968 - mse: 66.0767 - val_loss: 56.5111 - val_mse: 63.5693\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 63.5136 - mse: 66.0270 - val_loss: 56.8976 - val_mse: 63.9316\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 67.2761 - mse: 66.9787 - val_loss: 57.6333 - val_mse: 64.6640\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 64.9344 - mse: 65.4096 - val_loss: 55.1009 - val_mse: 62.0910\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 65.2902 - mse: 64.8494 - val_loss: 54.8330 - val_mse: 61.8026\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 63.0625 - mse: 64.2272 - val_loss: 55.0363 - val_mse: 61.9704\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 62.8610 - mse: 64.6769 - val_loss: 55.9658 - val_mse: 62.9269\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 64.9252 - mse: 64.4778 - val_loss: 54.4299 - val_mse: 61.3311\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 64.3592 - mse: 64.0480 - val_loss: 53.7256 - val_mse: 60.5846\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 63.1701 - mse: 62.8204 - val_loss: 53.7258 - val_mse: 60.5666\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 62.8254 - mse: 62.5361 - val_loss: 53.5967 - val_mse: 60.4077\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 62.7053 - mse: 62.3555 - val_loss: 53.2408 - val_mse: 60.0181\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 62.8687 - mse: 62.9662 - val_loss: 52.7411 - val_mse: 59.4743\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 62.2770 - mse: 61.8914 - val_loss: 52.3077 - val_mse: 58.9454\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 61.7550 - mse: 61.9400 - val_loss: 52.8869 - val_mse: 59.4790\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 61.3932 - mse: 61.4895 - val_loss: 51.8179 - val_mse: 58.4221\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 61.5066 - mse: 61.0704 - val_loss: 51.5642 - val_mse: 58.1135\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 60.9910 - mse: 60.8402 - val_loss: 51.5830 - val_mse: 58.0966\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 59.8947 - mse: 60.3324 - val_loss: 51.0767 - val_mse: 57.5814\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 60.3336 - mse: 60.7176 - val_loss: 50.9961 - val_mse: 57.5224\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 60.4196 - mse: 60.4916 - val_loss: 50.7662 - val_mse: 57.2466\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 60.0618 - mse: 59.7306 - val_loss: 51.0932 - val_mse: 57.5630\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 58.6098 - mse: 59.5505 - val_loss: 50.6296 - val_mse: 57.0624\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 58.2159 - mse: 59.3107 - val_loss: 50.1548 - val_mse: 56.5855\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.9945 - mse: 59.3920 - val_loss: 49.9360 - val_mse: 56.3253\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 58.9934 - mse: 60.4171 - val_loss: 50.0002 - val_mse: 56.3741\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 60.0925 - mse: 59.7794 - val_loss: 49.3942 - val_mse: 55.7415\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 59.3938 - mse: 59.0628 - val_loss: 49.8701 - val_mse: 56.1627\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.1892 - mse: 58.0563 - val_loss: 49.2880 - val_mse: 55.5838\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.9376 - mse: 57.7394 - val_loss: 48.9782 - val_mse: 55.2903\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.8634 - mse: 57.4417 - val_loss: 48.8964 - val_mse: 55.1963\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.5038 - mse: 57.2206 - val_loss: 48.6929 - val_mse: 54.9784\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.3658 - mse: 56.9926 - val_loss: 48.3413 - val_mse: 54.5902\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.4356 - mse: 57.0302 - val_loss: 48.3639 - val_mse: 54.5989\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 56.6009 - mse: 56.9058 - val_loss: 47.9718 - val_mse: 54.2022\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 56.3288 - mse: 57.1204 - val_loss: 50.0714 - val_mse: 56.3532\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 58.3716 - mse: 58.5413 - val_loss: 50.3323 - val_mse: 56.6481\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.4148 - mse: 57.1600 - val_loss: 48.2194 - val_mse: 54.4939\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 55.5358 - mse: 55.5347 - val_loss: 47.3822 - val_mse: 53.5766\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 56.6786 - mse: 56.2630 - val_loss: 47.5916 - val_mse: 53.7827\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 55.3281 - mse: 55.3505 - val_loss: 47.4315 - val_mse: 53.5839\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 55.3251 - mse: 55.9185 - val_loss: 47.4460 - val_mse: 53.5963\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 54.0289 - mse: 55.5782 - val_loss: 46.8978 - val_mse: 52.9548\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 56.0835 - mse: 55.9549 - val_loss: 46.3302 - val_mse: 52.3519\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 54.9292 - mse: 54.5966 - val_loss: 46.8324 - val_mse: 52.8393\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 55.0991 - mse: 54.8430 - val_loss: 46.6030 - val_mse: 52.5760\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 54.7311 - mse: 54.3977 - val_loss: 45.8081 - val_mse: 51.7160\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 54.0386 - mse: 53.8901 - val_loss: 46.3582 - val_mse: 52.2899\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.3594 - mse: 53.8537 - val_loss: 45.8033 - val_mse: 51.7280\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 54.8000 - mse: 54.4441 - val_loss: 45.5446 - val_mse: 51.4491\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.8116 - mse: 53.3514 - val_loss: 45.6115 - val_mse: 51.5251\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.8398 - mse: 53.6184 - val_loss: 45.1792 - val_mse: 51.0226\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 53.3677 - mse: 53.2410 - val_loss: 45.7237 - val_mse: 51.5631\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.3228 - mse: 53.0641 - val_loss: 44.9459 - val_mse: 50.7449\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.0089 - mse: 52.8378 - val_loss: 44.8148 - val_mse: 50.5935\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 52.3362 - mse: 52.5103 - val_loss: 45.2836 - val_mse: 51.0818\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.0092 - mse: 52.7699 - val_loss: 45.3376 - val_mse: 51.1687\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.8551 - mse: 52.3917 - val_loss: 44.7208 - val_mse: 50.5509\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.1359 - mse: 52.7729 - val_loss: 44.5765 - val_mse: 50.3873\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 52.9470 - mse: 52.6065 - val_loss: 44.7944 - val_mse: 50.6571\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.9509 - mse: 52.2906 - val_loss: 44.9116 - val_mse: 50.7767\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.6899 - mse: 51.5912 - val_loss: 44.1721 - val_mse: 49.9713\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 52.2123 - mse: 51.8067 - val_loss: 44.1784 - val_mse: 49.9996\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.9084 - mse: 51.5439 - val_loss: 44.2705 - val_mse: 50.0916\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.8499 - mse: 51.4348 - val_loss: 44.1947 - val_mse: 49.9597\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.2366 - mse: 51.0985 - val_loss: 43.6832 - val_mse: 49.3794\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.3357 - mse: 50.9882 - val_loss: 43.4914 - val_mse: 49.1487\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.0966 - mse: 50.7707 - val_loss: 43.5907 - val_mse: 49.2285\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.3291 - mse: 50.7888 - val_loss: 43.1613 - val_mse: 48.7453\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 52.3366 - mse: 52.2041 - val_loss: 43.4062 - val_mse: 48.9167\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.3232 - mse: 51.2256 - val_loss: 43.3756 - val_mse: 48.9265\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.8019 - mse: 50.4906 - val_loss: 43.0341 - val_mse: 48.5721\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.3790 - mse: 50.1247 - val_loss: 42.7436 - val_mse: 48.2485\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.7245 - mse: 50.5344 - val_loss: 42.6909 - val_mse: 48.2298\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.1988 - mse: 50.4714 - val_loss: 43.5598 - val_mse: 49.1215\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.9189 - mse: 49.7698 - val_loss: 43.0406 - val_mse: 48.4621\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 52.1455 - mse: 51.7860 - val_loss: 42.2960 - val_mse: 47.6300\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 51.0167 - mse: 50.6151 - val_loss: 43.6249 - val_mse: 48.9738\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.3437 - mse: 50.0281 - val_loss: 42.1641 - val_mse: 47.4755\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.2339 - mse: 49.9534 - val_loss: 41.9761 - val_mse: 47.2792\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.3386 - mse: 49.3767 - val_loss: 42.9495 - val_mse: 48.3245\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.6601 - mse: 49.3780 - val_loss: 42.3232 - val_mse: 47.6699\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 49.1027 - mse: 49.0849 - val_loss: 41.9101 - val_mse: 47.2377\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.4540 - mse: 48.8663 - val_loss: 41.7042 - val_mse: 46.9678\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.2837 - mse: 49.2289 - val_loss: 42.5341 - val_mse: 47.8045\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.7247 - mse: 48.4203 - val_loss: 41.4594 - val_mse: 46.6418\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.3915 - mse: 49.3665 - val_loss: 41.3485 - val_mse: 46.5297\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.3791 - mse: 49.0466 - val_loss: 41.2522 - val_mse: 46.4228\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 46.7548 - mse: 48.1073 - val_loss: 41.8907 - val_mse: 47.1146\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.2794 - mse: 48.1429 - val_loss: 41.1542 - val_mse: 46.3712\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 48.4899 - mse: 48.2617 - val_loss: 41.1230 - val_mse: 46.3575\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.9655 - mse: 47.9775 - val_loss: 41.5211 - val_mse: 46.7651\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.6406 - mse: 48.2407 - val_loss: 41.2312 - val_mse: 46.4051\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.1691 - mse: 48.8613 - val_loss: 40.8591 - val_mse: 45.9787\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.2795 - mse: 48.1929 - val_loss: 41.7889 - val_mse: 46.9951\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.8571 - mse: 47.7461 - val_loss: 40.7263 - val_mse: 45.8362\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.9577 - mse: 47.5003 - val_loss: 41.3974 - val_mse: 46.5608\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.2230 - mse: 47.3116 - val_loss: 40.5635 - val_mse: 45.7028\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.5299 - mse: 47.2930 - val_loss: 40.5236 - val_mse: 45.6640\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.4990 - mse: 47.2264 - val_loss: 40.5645 - val_mse: 45.7314\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 47.3123 - mse: 47.0591 - val_loss: 40.9091 - val_mse: 46.1180\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.0424 - mse: 47.0168 - val_loss: 40.4564 - val_mse: 45.5955\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.2314 - mse: 46.9323 - val_loss: 40.3871 - val_mse: 45.4688\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.3124 - mse: 47.1373 - val_loss: 40.8604 - val_mse: 45.9419\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.6206 - mse: 46.7905 - val_loss: 40.3300 - val_mse: 45.3467\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.2001 - mse: 46.9518 - val_loss: 40.5580 - val_mse: 45.5547\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.9529 - mse: 46.6957 - val_loss: 40.0980 - val_mse: 45.0668\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.3863 - mse: 47.1863 - val_loss: 40.0094 - val_mse: 44.9449\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.6151 - mse: 46.7157 - val_loss: 40.1628 - val_mse: 45.1548\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.0255 - mse: 46.6695 - val_loss: 40.4212 - val_mse: 45.4526\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 46.5569 - mse: 46.3031 - val_loss: 39.8728 - val_mse: 44.8016\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.5367 - mse: 46.1985 - val_loss: 39.7494 - val_mse: 44.6538\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 46.2290 - mse: 46.1077 - val_loss: 39.8882 - val_mse: 44.8159\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 46.3368 - mse: 46.0015 - val_loss: 40.2160 - val_mse: 45.1970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa38bb38eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFu2k4J_spfi",
        "colab_type": "code",
        "outputId": "e5c9bc58-78b7-43c3-fa28-a79c3d7cc1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 54.4169 - mse: 54.7650\n",
            "Mean Squared Error - Test Data 54.765007\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}